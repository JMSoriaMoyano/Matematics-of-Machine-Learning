{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del método del gradiente descendiente para la regresión lineal multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Math,Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la función lineal multivarible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de una función lineal multivarible, que estima un valor deseado, por ejemplo el valor de una vivienda, dada multiples característica (m2 de la vivenda, número de habitaciones,étc), viene dada por la expresión: \n",
    "\n",
    "$$\\\\h_\\omega(x)=\\omega_0+\\omega_1x_1+\\omega_2x_2+\\omega_3x_3+...+\\omega_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde:\n",
    "    *n* es el número de características (*features*) o variables predictoras; \n",
    "     $[x_1,x_2,..x_j,..x_n]$ son los valores  de las variables predictorias; \n",
    "     $[\\omega_1,\\omega_2,..,\\omega_j,..\\omega_n]$ son los parámetros que definen la función lineal, por ejemplo el $\\omega_1$:precio por metro cuadrado o $\\omega_2$:el precio por habitación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma compacta de escribir la función, es utilizando el producto interno o producto escalar de dos vectores. Sea $x$ el vector (fila) del valor de las variables predictoras $[x_0,x_1,x_2,..x_j,..x_n]$ , donde $x_0=1$, y $\\omega^T$ el vector columna de los parámetros de la función lineal. En estas condiciones $x,\\omega\\in[1\\times(n+1)]$, y la función multivariable linear se puede escribir como:\n",
    "$$h_\\omega(x)=x\\cdot \\omega^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del problema de regresión lineal Multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si disponemos de una base de datos de *n+1* campos, en la que, para cada instancia *(i)*, existen *n* variables predictorias $[x_{1}^{(i)},x_{2}^{(i)},..x_{j}^{(i)},..x_{n}^{(i)}]$ y una variable predecible $y^{(i)}$. Dadas las  *m* instancias  (*training examples*), el problema se define como: \"Encontrar los parámetos, de la función lineal mutilvarible $[\\omega_1,\\omega_2,..,\\omega_j,..\\omega_n]$ ($\\omega$), que mejor ajusten el comportamiento de las variables predictorias. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma matricial de escribir las *m* relaciones lineales multivaribles, de la base de datos es:\n",
    "\n",
    "$$X\\cdot\\omega^T=y$$\n",
    "\n",
    "donde $X\\in[m\\times(n+1)]$, $\\omega\\in[1\\times(n+1)]$ y $y\\in[(n+1)\\times1]$. $\\forall i\\in\\{1..m\\} \\rightarrow x_0^{(i)}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Coste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función de coste como la suma de las diferencias al cuadrado, para todas las instancias de entrenamiento de la base de datos, entre el valor real de la variable predecible y el valor calculado a partir la función lineal multivariable, a partir de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\omega)=\\frac{1}{2m}\\sum_{i=1}^m(h_\\omega(x^{(i)})-y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente descendiente para multiples variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vector de parámetros $w$ que mejor ajusta el comportamiento de las variables predictorias es aquel que hace mínima la función de coste. Esto es \n",
    "$$\\frac{dJ}{d\\omega}=0$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $J$ es una función multivariable escalar, su derivada se obtiene a través del operador\n",
    "$\\nabla$. $\\nabla J$ es una función vectorial, por lo que al igualarla a 0, ($\\nabla J=0$) se establecen n ecuaciones de donde optener los n parámetros del vector $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla J=0 \\longleftrightarrow \\omega_j=\\frac{1}{m}\\sum_{i=1}^m(x^{(i)}\\cdot\\omega^T-y^{(i)})x_j^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
